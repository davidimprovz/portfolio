{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anvil Real Estate (Demo)*\n",
    "(c) 2018 David Williams. All Rights Reserved\n",
    "\n",
    "**What**: A data science app that evaluates properties to identify parcels with a likelihood (0-1, where 1=100%) of profitability (unprofitable vs profitable when sold). \n",
    "\n",
    "In this demo, the app evaluates Hawaii County, HI. The code derives a machine learning model from past sales history and broader economic conditions. Then the model is used to assign a likelihood of sales outcomes to each parcel in the current year using up-to-date economic measures and most recent sales figures. These predictions can be used to symbolize real estate on a map and evaluate investment opportunities at a glance.\n",
    "\n",
    "**Use**: Allows a generic investor with no prior knowledge of a market to quickly determine a property's *likely* economic status, lowering the information barrier of entry to a real estate market.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "When selecting \"Run All Cells\" above, the average run time will take approximately 20 min. Results will be displayed in an ArcGIS map widget at the bottom of the notebook. \n",
    "\n",
    "1. <a href='#Step-1:-Get-the-Data'>Acquire data</a> \n",
    "\n",
    "2. <a href='#Step-2:-Inspect-Data-Distributions'>Inspect Data Distributions</a>\n",
    "\n",
    "3. <a href='#Step-3:-Create-Machine-Learning-Model-(Supervised)'>Create machine learning (ML) model</a>\n",
    "\n",
    "4. <a href='#Run-ML-Model-and-Score'>Run ML model backtest and make current status predictions</a>\n",
    "\n",
    "5. <a href='#Step-5:-Visualize-Results-with-ArcGIS'>Display properties in a map, symbolized by likelihood of profitability</a>\n",
    "\n",
    "\n",
    "\n",
    "\\* This software is for demo purposes. Signifigant portions of data and other programming routines have been omitted from this demo. Prediction results from the full application can vary. If you would like to know more or have a feature request, please write to <a href='mailto:info@improvz.com'>info@improvz.com</a> and include the subject line 'Anvil Data App'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, os, re, datetime, time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3 \n",
    "\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "\n",
    "from modules.anvil_mods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SALE HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to db. Note: acquisition routines for this data not provided with this demo app.\n",
    "dbcnx = sqlite3.connect('data/hi_county_parcels.db')\n",
    "# load the data\n",
    "hi_sales = pd.read_sql('SELECT * FROM sample_data;', con=dbcnx).drop(['index'], axis=1)\n",
    "# close db connection \n",
    "dbcnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parcel num to int\n",
    "hi_sales['parcel'] = hi_sales['parcel'].astype(int) \n",
    "# convert all dates to dt\n",
    "hi_sales['Sale Date'] = pd.to_datetime(hi_sales['Sale Date'], errors='coerce')\n",
    "# convert sale amt to float\n",
    "hi_sales['Sale Amount'] = hi_sales['Sale Amount'].str.replace('[$,]','').astype(float)\n",
    "# use only sales where amt > 0\n",
    "hi_sales = hi_sales[hi_sales['Sale Amount'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first and last dates of all sale dates\n",
    "start_date = hi_sales[hi_sales['Sale Date'].dt.year > 1900].min()['Sale Date']\n",
    "end_date = hi_sales[hi_sales['Sale Date'].dt.year > 1900].max()['Sale Date']\n",
    "# mask away sale dates for which start date <= x <= end date  \n",
    "mask = (hi_sales['Sale Date'] >= start_date) & (hi_sales['Sale Date'] <= end_date)\n",
    "hi_sales = hi_sales[mask]\n",
    "# free memory\n",
    "del(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate sales, identified by date and parcel\n",
    "hi_sales.drop_duplicates(subset=['Sale Date', 'parcel'], inplace=True)\n",
    "# remove records without a sale amount\n",
    "hi_sales = hi_sales[~hi_sales['Sale Amount'].isin([np.NaN])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index with all parcels that have repeat sales history\n",
    "test = hi_sales['parcel'].value_counts().gt(1)\n",
    "# filter out only True values\n",
    "idx = test[test==True].index\n",
    "# create a container with all repeat sales\n",
    "parcels_w_hist = hi_sales[hi_sales['parcel'].isin(idx)]\n",
    "# free memory\n",
    "del(hi_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a column to indicate profitability\n",
    "parcels_w_hist['Profitable Sale'] = 0\n",
    "\n",
    "# determine the sale profitability status by:\n",
    "# sort parcels and dates so most recent items are first for comparison\n",
    "parcels_w_hist.sort_values(['parcel','Sale Date'], ascending=False, inplace=True)\n",
    "# create a tf table\n",
    "tf = parcels_w_hist['Sale Amount'] > parcels_w_hist['Sale Amount'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean index for last items\n",
    "\n",
    "# first make an empty list\n",
    "last_parcels = []\n",
    "# get list of unique parcels\n",
    "unique_parcels = parcels_w_hist['parcel'].unique()\n",
    "\n",
    "# for each parcel\n",
    "for parcel in unique_parcels: \n",
    "    # isolate parcel sale history within a separate df\n",
    "    idx = parcels_w_hist[parcels_w_hist['parcel'] == parcel].iloc[-1].name\n",
    "    # append the parcel index\n",
    "    last_parcels.append(idx)\n",
    "\n",
    "# remove items from tf where item is last in index\n",
    "mask_tf = (~tf.index.isin(last_parcels)) & (tf == True)\n",
    "# select the parcels using mask and set profitable sale to 1\n",
    "parcels_w_hist.loc[mask_tf, 'Profitable Sale'] = 1\n",
    "# free memory\n",
    "del(unique_parcels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### County Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data products\n",
    "HI_geo_stem = 'https://opendata.arcgis.com/datasets/'\n",
    "HI_parcels_json = '1eb5fa03038d49cba930096ea67194e0_5'\n",
    "HI_zoning_json = 'e81bcaf8da9a4140a09aa922ef918ea4_2' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARCEL DATA AS GEOJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create download link\n",
    "parcels_link = ''.join([HI_geo_stem, HI_parcels_json, '.geojson'])\n",
    "# get json\n",
    "raw_parcels_json = requests.get(parcels_link).json()\n",
    "# load into geopandas\n",
    "hi_parcels = convertGeoJsonGeometry(raw_parcels_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate parcel names\n",
    "hi_parcels['name'] = hi_parcels.qpub_link.str.extract('(?P<parcel>\\d{10,25})').astype(int)\n",
    "# extract desired cols\n",
    "columns = ['geometry', 'bldgexempt', 'bldgvalue', 'gisacres', \n",
    "           'homeowner', 'landexempt', 'landvalue', 'name', 'pittcode', \n",
    "           'st_areashape', 'taxacres']\n",
    "hi_parcels = hi_parcels[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del(parcels_link, raw_parcels_json, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZONING DATA AS GEOJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoning \n",
    "zoning_link = ''.join([HI_geo_stem, HI_zoning_json, '.geojson'])\n",
    "# get json\n",
    "raw_zones_json = requests.get(zoning_link).json()\n",
    "# get only desired cols\n",
    "columns = ['zone', 'zoning_id','district']\n",
    "# load into geopandas\n",
    "hi_zones_gpd = convertSpecGeoJsonGeometry(raw_zones_json, columns)\n",
    "# remove all roads \n",
    "hi_zones_gpd = hi_zones_gpd[~hi_zones_gpd['zone'].str.contains('road')] \n",
    "# remove zone column\n",
    "hi_zones_gpd.drop(['zone'], axis=1, inplace=True)\n",
    "# free memory\n",
    "del(raw_zones_json, columns, zoning_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatially join features: parcels with zones\n",
    "hi_parcels_w_zones = spatialJoinFeatures(hi_parcels, hi_zones_gpd) # !! about 10min runtime on single core !!\n",
    "# clean up cols\n",
    "hi_parcels_w_zones.columns = hi_parcels_w_zones.columns.str.replace('properties.','')\n",
    "# drop those parcels that have more than one entry\n",
    "# names = hi_parcels_w_zones['name'].value_counts().gt(1).index\n",
    "hi_parcels_w_zones.drop_duplicates(subset='name', keep=False, inplace=True)\n",
    "# free memory\n",
    "del(hi_parcels, hi_zones_gpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the names so they can be merged with hi_sales\n",
    "# change the name column to be matched wth hi_sales on a merge\n",
    "hi_parcels_w_zones.rename({'name':'parcel'}, inplace=True, axis=1)\n",
    "# format parcel numbers as ints so they match hi_sales\n",
    "hi_parcels_w_zones['parcel'] = hi_parcels_w_zones['parcel'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean & Sort HI County Tax Authority Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge hi_parcels_w_zones with sales \n",
    "parcels_w_sales = pd.merge(hi_parcels_w_zones, parcels_w_hist, how='inner', on='parcel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del(hi_parcels_w_zones, parcels_w_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SET SALE DATE INTERVALS FOR EACH PARCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index before determing sale date intervals\n",
    "parcels_w_sales.reset_index(drop=True, inplace=True)\n",
    "# set intervals\n",
    "parcels_w_sales['Sale Intvl'] = getDateIntvlByParcel(parcels_w_sales[['parcel', 'Sale Date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD OTHER LAND & BUILDING INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new connection\n",
    "dbcnx= sqlite3.connect('data/hi_county_parcels.db')\n",
    "# get parcel data\n",
    "parceldat = pd.read_sql('SELECT * FROM parceldat', con=dbcnx)\n",
    "dbcnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all digits to float or int\n",
    "parceldat['land_area'] = parceldat['land_area'].str.replace('[,]', '').astype(float)\n",
    "parceldat['mkt_bld_val'] = parceldat['mkt_bld_val'].str.replace('[,$]', '').astype(float)\n",
    "parceldat['mkt_land_val'] = parceldat['mkt_land_val'].str.replace('[,$]', '').astype(float)\n",
    "parceldat['tot_tax_val'] = parceldat['tot_tax_val'].str.replace('[,$]', '').astype(float)\n",
    "parceldat['parcel'] = parceldat['parcel'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge parcels_w_hist with parceldat\n",
    "parcels_w_sales = parcels_w_sales.merge(parceldat, on='parcel')\n",
    "# free memory\n",
    "del(parceldat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach State-wide Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUFFER DISTANCE FOR EACH PARCEL TO GET SPATIAL INTERSECTION WITH FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting degrees to miles for buffer distance\n",
    "deg_to_mi = .001 * 14.4569170002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove parcels w/o a geometry\n",
    "parcels_w_sales = parcels_w_sales[~parcels_w_sales['geometry'].isnull()]\n",
    "# create a 1mi radius around each property.. * n | n=desired miles.\n",
    "parcels_w_sales['buff_dist'] = gpd.GeoDataFrame(parcels_w_sales.buffer(deg_to_mi))\n",
    "# set the new geometry for calculations\n",
    "parcels_w_sales = parcels_w_sales.set_geometry('buff_dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPATIAL INTERSECTION: ROADS & OTHER INFRASTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roads supplied by Hawaii GIS dept.\n",
    "hi_roads_all = '636620e50cce4b44820c545eb3195a31_6' # all roads\n",
    "hi_roads_st = '1598e6509d574dceafb939a945a2075b_11' # state\n",
    "hi_roads_maj = 'c32f8d888edc4f1fa428973f785f9eee_8' # major "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make road links\n",
    "hi_roads_all_link = ''.join([HI_geo_stem, hi_roads_all, '.geojson'])\n",
    "hi_roads_st_link = ''.join([HI_geo_stem, hi_roads_st, '.geojson'])\n",
    "hi_roads_maj_link = ''.join([HI_geo_stem, hi_roads_maj, '.geojson'])\n",
    "\n",
    "# acquire all roads data \n",
    "hi_roads_all_json = requests.get(hi_roads_all_link).json()\n",
    "# convert to geopandas\n",
    "hi_roads_all_gpd = convertSpecGeoJsonGeometry(hi_roads_all_json, ['fullname', 'featureuid'])\n",
    "# rename col\n",
    "hi_roads_all_gpd.rename({'fullname':'road'}, inplace=True, axis=1)\n",
    "\n",
    "# acquire state roads\n",
    "hi_roads_st_json = requests.get(hi_roads_st_link).json()\n",
    "# convert to geopandas\n",
    "hi_roads_st_gpd = convertSpecGeoJsonGeometry(hi_roads_st_json, ['route_name', 'island', 'featureuid'])\n",
    "# extract hi only \n",
    "hi_roads_st_gpd = hi_roads_st_gpd[hi_roads_st_gpd['island'] == 'Hawaii'].drop(['island'], axis=1)\n",
    "# rename col\n",
    "hi_roads_st_gpd.rename({'route_name':'road'}, inplace=True, axis=1)\n",
    "\n",
    "# acquire major routes\n",
    "hi_roads_maj_json = requests.get(hi_roads_maj_link).json()\n",
    "# convert to geopandas\n",
    "hi_roads_maj_gpd = convertSpecGeoJsonGeometry(hi_roads_maj_json, ['fename', 'featureuid'])\n",
    "# rename col\n",
    "hi_roads_maj_gpd.rename({'fename':'road'}, inplace=True, axis=1)\n",
    "\n",
    "# concat all roads into single record\n",
    "all_roads_gpd = pd.concat([hi_roads_all_gpd, hi_roads_st_gpd, hi_roads_maj_gpd], sort=True)\n",
    "# free memory\n",
    "del(hi_roads_all, hi_roads_st, hi_roads_maj,\n",
    "    hi_roads_all_link, hi_roads_st_link, hi_roads_maj_link,\n",
    "    hi_roads_all_json, hi_roads_st_json, hi_roads_maj_json,\n",
    "    hi_roads_all_gpd, hi_roads_st_gpd, hi_roads_maj_gpd,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do buffer for each parcel and count number w/in proximity\n",
    "parcels_w_roads = spatialJoinFeatures(parcels_w_sales[['parcel', 'buff_dist']], all_roads_gpd) #!! 10min run time\n",
    "# for each parcel, create a count table and use that for lookup\n",
    "# then add column for num banks within radius and apply count\n",
    "parcels_w_counts = getCountForSpatialJoin(parcels_w_sales['parcel'], parcels_w_roads, 'road', 'featureuid')\n",
    "# free up memory\n",
    "del(all_roads_gpd, parcels_w_roads)\n",
    "# add count column to parcels_w_hist for roads nearby.\n",
    "parcels_w_sales['roads_nearby'] = parcels_w_sales['parcel'].apply(lambda x: parcels_w_counts.get(x)) \n",
    "# free up memory\n",
    "del(parcels_w_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIGITAL ELEVATION MODEL DERIVED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: http://www.soest.hawaii.edu/coasts/data/hawaii/dem.html\n",
    "\n",
    "# load pre-computed elevation data from CSV\n",
    "dem_stats = pd.read_csv('data/hi_parcel_elev_slope_stats.csv').drop(['AREA', 'SUM', 'SUM_1','COUNT'], axis=1)\n",
    "# rename cols\n",
    "col_names = {\n",
    "    'MIN':'ELEV_MIN',\n",
    "    'MAX':'ELEV_MAX',\n",
    "    'RANGE':'ELEV_RANGE',\n",
    "    'MEAN':'ELEV_MEAN',\n",
    "    'STD':'ELEV_STD',\n",
    "    'MIN_1':'SLOPE_MIN',\n",
    "    'MAX_1':'SLOPE_MAX',\n",
    "    'RANGE_1':'SLOPE_RANGE',\n",
    "    'MEAN_1':'SLOPE_MEAN',\n",
    "    'STD_1':'SLOPE_STD',\n",
    "}\n",
    "dem_stats.rename(col_names, axis=1, inplace=True)\n",
    "# set the dtype for parcels\n",
    "dem_stats['parcel'] = dem_stats['parcel'].astype(int)\n",
    "# merge dem stats with parcels_w_hist by parcel number\n",
    "parcels_w_sales = pd.merge(parcels_w_sales, dem_stats, on='parcel', how='inner')\n",
    "# free up memory\n",
    "del(dem_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPATIAL INTERSECTION: SCHOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make public and private school json links\n",
    "school_link_pub = ''.join([HI_geo_stem, 'ba92e2fc0d2d4497af7289cb4ab552a7_7', '.geojson'])\n",
    "school_link_priv = ''.join([HI_geo_stem, '95e39721d53c4319a5a4525866515458_8', '.geojson'])\n",
    "\n",
    "# filter desired cols\n",
    "priv_cols = ['island', 'address', 'school', 'students11', 'tuition11', 'zipcode']\n",
    "pub_cols = ['district', 'address', 'city', 'esis_name', 'island', 'objectid', 'grade_from', 'grade_to', 'name', 'zip', 'type']\n",
    "\n",
    "columns = ['name', 'island', 'geometry', 'address'] \n",
    "\n",
    "# get pub school json\n",
    "hi_pubschool_json = requests.get(school_link_pub).json()\n",
    "# convert to geopandas\n",
    "hi_pubschool_gpd = convertSpecGeoJsonGeometry(hi_pubschool_json, pub_cols) \n",
    "# filter desired cols\n",
    "hi_pubschool_gpd = hi_pubschool_gpd[columns]\n",
    "\n",
    "# get priv school json\n",
    "hi_privschool_json = requests.get(school_link_priv).json()\n",
    "# convert to geopandas\n",
    "hi_privschool_gpd = convertSpecGeoJsonGeometry(hi_privschool_json, priv_cols)\n",
    "# rename the hi_privschool_gpd 'name' col to \"school\" \n",
    "hi_privschool_gpd.rename({'school':'name'}, inplace=True, axis=1)\n",
    "# filter desired cols\n",
    "hi_privschool_gpd = hi_privschool_gpd[columns]\n",
    "\n",
    "# join the public and private school data \n",
    "hi_schools_gpd = pd.concat([hi_privschool_gpd, hi_pubschool_gpd])\n",
    "# filter island to Hawaii\n",
    "hi_schools_gpd = hi_schools_gpd[hi_schools_gpd['island'] == 'Hawaii']\n",
    "# keep only needed cols\n",
    "hi_schools_gpd = hi_schools_gpd[['geometry', 'name', 'address']]\n",
    "\n",
    "# do buffer for each parcel and count number w/in proximity\n",
    "parcels_w_schools = spatialJoinFeatures(parcels_w_sales[['parcel', 'buff_dist']], hi_schools_gpd)\n",
    "# for each parcel, create a count table and use that for lookup\n",
    "# then add column for num banks within radius and apply count\n",
    "parcels_w_counts = getCountForSpatialJoin(parcels_w_sales['parcel'], parcels_w_schools, 'name', 'address')\n",
    "# series with counts of roads within radius of each parcel\n",
    "\n",
    "# free up memory\n",
    "del(\n",
    "    school_link_priv, school_link_pub, \n",
    "    priv_cols, pub_cols, columns, \n",
    "    hi_pubschool_json, hi_privschool_json, \n",
    "    hi_pubschool_gpd, hi_privschool_gpd,\n",
    "    hi_schools_gpd\n",
    "   )\n",
    "\n",
    "# add the count to parcels_w_hist\n",
    "parcels_w_sales['schools_nearby'] = parcels_w_sales.parcel.apply(lambda x: parcels_w_counts.get(x))\n",
    "\n",
    "# free up memory\n",
    "del(parcels_w_counts, parcels_w_schools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Federal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "from fred import Fred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels_w_sales['Year'] = parcels_w_sales['Sale Date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEDERAL RESERVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRED: fed funds / interbank lending rate: https://fred.stlouisfed.org/series/IR3TIB01USM156N\n",
    "\n",
    "# yield curves: https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield - daily yield curve\n",
    "fed_funds = quandl.get(\"FRED/DFF\", start_date=start_date, end_date=end_date) #.plot()\n",
    "# group fedfunds by year and extract the sum of the differences for each year\n",
    "fed_funds_mvt = getPeriodicIndexMovement(fed_funds)\n",
    "# get the sale year's fed funds record using a dict to lookup the value\n",
    "parcels_w_sales['Fed Funds'] = parcels_w_sales['Year'].apply(lambda x: fed_funds_mvt.get(x))\n",
    "# free memory by removing unneeded records\n",
    "del(fed_funds, fed_funds_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUANDL case-shiller index\n",
    "# https://www.quandl.com/data/FED/FL075035223_A-Interest-rates-and-price-indexes-owner-occupied-real-estate-S-P-Case-Shiller-national-SA-Annual-Levels-NSA\n",
    "\n",
    "# get the case_shiller = quandl.get(\"FED/FL075035223_A\", start_date=start_date, end_date=end_date)\n",
    "case_shill_idx = formatIndicatorLikeQuandl('CSUSHPINSA')\n",
    "# group the movement and get the sum of differences for each year\n",
    "case_shill_mvt = getPeriodicIndexMovement(case_shill_idx)\n",
    "# set the sale year's case shill number and related stats by dict lookup\n",
    "parcels_w_sales['CaseShillMvt'] = parcels_w_sales['Year'].apply(lambda x: case_shill_mvt.get(x))\n",
    "# free memory\n",
    "del(case_shill_idx, case_shill_mvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CENSUS BUREAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLRESCONS: Total Res Construction Spending\n",
    "\n",
    "# get the index\n",
    "res_const = quandl.get(\"FRED/TLRESCONS\", start_date=start_date, end_date=end_date) \n",
    "# group the years and get the sum of the differences for each year\n",
    "res_const_mvt = getPeriodicIndexMovement(res_const)\n",
    "# set the non res const value for each sale using a dict lookup\n",
    "parcels_w_sales['ResConst'] = parcels_w_sales['Year'].apply(lambda x: res_const_mvt.get(x))\n",
    "# free memory\n",
    "del(res_const, res_const_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLPRVCONS: total private construction spending\n",
    "\n",
    "# get the index\n",
    "tot_priv_const = quandl.get(\"FRED/TLPRVCONS\", start_date=start_date, end_date=end_date) \n",
    "# group the years and get the sum of the differences for each year\n",
    "tot_priv_const_mvt = getPeriodicIndexMovement(tot_priv_const)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['PrivConst'] = parcels_w_sales['Year'].apply(lambda x: tot_priv_const_mvt.get(x))\n",
    "# free memory\n",
    "del(tot_priv_const, tot_priv_const_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WPUSI012011: construction price index\n",
    "\n",
    "# get the index\n",
    "con_price_idx = quandl.get(\"FRED/WPUSI012011\", start_date=start_date, end_date=end_date)\n",
    "# get index movement\n",
    "con_price_idx_mvt = getPeriodicIndexMovement(con_price_idx)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['ConstPriceIdx'] = parcels_w_sales['Year'].apply(lambda x: con_price_idx_mvt.get(x))\n",
    "# free memory\n",
    "del(con_price_idx, con_price_idx_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIHAWA0URN: hi county jobless claims\n",
    "\n",
    "# get the index\n",
    "hi_cty_jobless = quandl.get(\"FRED/HIHAWA0URN\", start_date=start_date, end_date=end_date)\n",
    "# get index movement\n",
    "hi_cty_jobless_mvt = getPeriodicIndexMovement(hi_cty_jobless)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['HiCtyJobless'] = parcels_w_sales['Year'].apply(lambda x: hi_cty_jobless_mvt.get(x))\n",
    "# free memory\n",
    "del(hi_cty_jobless, hi_cty_jobless_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCPI15001: per capita personal income, hawaii county\n",
    "\n",
    "# get the index\n",
    "hi_cty_percapinc = quandl.get(\"FRED/PCPI15001\", start_date=start_date, end_date=end_date)\n",
    "# get index movement\n",
    "hi_cty_percapinc_mvt = getPeriodicIndexMovement(hi_cty_percapinc)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['PerCapInc'] = parcels_w_sales['Year'].apply(lambda x: hi_cty_percapinc_mvt.get(x))\n",
    "# free memory\n",
    "del(hi_cty_percapinc, hi_cty_percapinc_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSN1F: new one fam home sales, all US\n",
    "\n",
    "# get the index\n",
    "one_fam_sales = quandl.get(\"FRED/HSN1F\", start_date=start_date, end_date=end_date)\n",
    "# get index movement\n",
    "one_fam_sales_mvt = getPeriodicIndexMovement(one_fam_sales)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['1FamHomeSales'] = parcels_w_sales['Year'].apply(lambda x: one_fam_sales_mvt.get(x))\n",
    "# free memory\n",
    "del(one_fam_sales, one_fam_sales_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOWNRATEACS015001: home ownership, hawaii county\n",
    "\n",
    "# get the index\n",
    "hi_cty_homeown = formatIndicatorLikeQuandl('HOWNRATEACS015001')\n",
    "# get index movement\n",
    "hi_cty_homeown_mvt = getAnnualIndexMovement(hi_cty_homeown)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['HiCtyHomeown'] = parcels_w_sales['Year'].apply(lambda x: hi_cty_homeown_mvt.get(x))\n",
    "# free memory\n",
    "del(hi_cty_homeown, hi_cty_homeown_mvt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEDERAL HOUSING FINANCE AUTHORITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USSTHPI: All-Transactions-House-Price-Index-for-the-United-States\n",
    "\n",
    "# get the index\n",
    "hpi_us = quandl.get(\"FRED/USSTHPI\", start_date=start_date, end_date=end_date)\n",
    "# get index movement\n",
    "hpi_us_mvt = getPeriodicIndexMovement(hpi_us)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['HPI US'] = parcels_w_sales['Year'].apply(lambda x: hpi_us_mvt.get(x))\n",
    "# free memory\n",
    "del(hpi_us, hpi_us_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HISTHPI: All-Transactions-House-Price-Index-for-Hawaii\n",
    "\n",
    "# get the index\n",
    "hpi_hi = quandl.get(\"FRED/HISTHPI\", start_date=start_date, end_date=end_date)\n",
    "# get index movement\n",
    "hpi_hi_mvt = getPeriodicIndexMovement(hpi_hi)\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['HPI HI'] = parcels_w_sales['Year'].apply(lambda x: hpi_hi_mvt.get(x))\n",
    "# free memory\n",
    "del(hpi_hi, hpi_hi_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX30YR: Mortgage Interest Rate Index, US \n",
    "\n",
    "# get the index\n",
    "mort_interest = quandl.get(\"FMAC/FIX30YR\", start_date=start_date, end_date=end_date).loc[:,'US Interest Rate']\n",
    "# get index movement\n",
    "mort_interest_mvt = getPeriodicIndexMovement(pd.DataFrame(mort_interest).rename({'US Interest Rate':'Value'}, axis=1))\n",
    "# set value for each sale using a dict lookup\n",
    "parcels_w_sales['FredMacMortInt'] = parcels_w_sales['Year'].apply(lambda x: mort_interest_mvt.get(x))\n",
    "# free memory\n",
    "del(mort_interest, mort_interest_mvt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the year col\n",
    "parcels_w_sales.drop(['Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inspect Data Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLOPE MEAN 0-7\n",
    "\n",
    "The box plot targets a geographical feature of the real estate, Slope Mean.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Slope Mean observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 1261-1268 observations each, representing the entire range of values 0-20.\n",
    "\n",
    "Properties with Slope Mean 0 <= x <= 7 should be favored. There are many observations clustered around 12-25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] ==1]['SLOPE_MEAN'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELEV MEAN: 50-400\n",
    "\n",
    "The box plot targets a monetary feature of the real estate, Bldg Exempt.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Bldg Exempt observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 1262-1265 observations each, representing nearly the entire range of values 0-3964.\n",
    "\n",
    "Properties with Land Value 85 <= x <= 1000 should be favored with the main focus on those between 0-400. This suggests the market prefers property that is positioned at or slightly above the beach at elevations between 270-1300 feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] ==1]['ELEV_MEAN'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAND VALUE: <=250K\n",
    "\n",
    "The box plot targets a monetary feature of the land, Land Value.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Land Value observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 1300-1500 observations each, representing nearly the entire range of values 100k-2M.\n",
    "\n",
    "Properties with Land Value <=2M should be favored. The market favors those <=250k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['landvalue'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GISACRES: 1\n",
    "\n",
    "The box plot targets a geographical feature of the land, GIS Acers.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records.\n",
    "\n",
    "The GIS Acres observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case all bins have ~3000 observations each representing the entire range of values .01-2.7k.\n",
    "\n",
    "Properties with GIS Acerage .2 <= x <=1.5 should be favored with special emphasis on around 1 acre of land. Note that there are very few sales where acerage exceeds 1. Few large parcels are ever sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['gisacres'], bins=4, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAX ACRES: .5-1\n",
    "\n",
    "This box plot targets a geographcial feature of the real estate, Tax Acres.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Tax Acres observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 1k-1.8k observations each, representing nearly the entire range of values .002 - 2750.\n",
    "\n",
    "Properties with Tax Acres .1 <= x <= 1  should be favored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['taxacres'], bins=3, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOT TAX VAL: 150k\n",
    "\n",
    "This box plot targets a monetary feature of the real estate, Total Tax Val.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Total Tax Val observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 1.2k-1.5k observations each, representing nearly the entire range of values 27k-32M .\n",
    "\n",
    "Properties with Total Tax Val 100k <= x <= 250K should be favored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] ==1]['tot_tax_val'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MKT BLD VAL: $150k-$200k\n",
    "\n",
    "This box plot targets a monetary feature of the real estate, Mkt Bld Val.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Mkt Bld Val observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, one bin (<=466k) holds the majority of observations, representing nearly the entire range of values which consists of values <=$500k.\n",
    "\n",
    "Properties with Mkt Bld Val 125k <= x <=225K should be favored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] ==1]['mkt_land_val'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CASE SHILL MVT: 2-12\n",
    "\n",
    "The box plot targets an index feature of the economic state, The Case Shill Mvt.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Case Shill Mvt observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 3k-5k observations each, representing nearly the entire range of values 2-12.\n",
    "\n",
    "Case Shill Mvt of 2 <= x <= 12 should be favored with lulls between 12-15 and a spike between 15-20, possibly signaling hesitation as the market climbs and extatic behavior at the height of the housing market.\n",
    "\n",
    "There is a spike in sales at the 2-3 interval, A bulk of sales at the 8-10, and another spike at 11-12. Sales jump rather than plod.\n",
    "\n",
    "Overall, positive Case Shill Mvt signals more sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['CaseShillMvt'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONST PRICE IDX: 2-5\n",
    "\n",
    "The box plot targets an index feature of the economic state, The Const Price Idx Rate.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Const Price Idx observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 2.5k - 5.5k observations each, representing nearly the entire range of values 1.4 - 5.3.\n",
    "\n",
    "Const Price Idx rates 0 <= x <= 5 should be favored, with a small spike around 15. Within the range, there are spikes at precisely 2-3, 4, and 5. This suggests jumps in sales rather than plodding increases. It could also be an effect of monthly measurement procedures. An interpolated value would smooth, accounting for procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['ConstPriceIdx'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 FAM HOME SALES: ~50\n",
    "\n",
    "The box plot targets an index feature of the economic state, The Single Family Home Sales Rate.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Single Family Home Sales observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 3k-6.5k observations each, representing nearly the entire range of values -19 to 77.\n",
    "\n",
    "Single Family Home Sales rates 0 <= x <= 100 should be favored. A sharp spike occurs at around 50. This shows that the Hawaiian housing market is positively coorelated to the broader economy of home sales. When the home sales rate increases, profitability increases. Thus, the Hawaiian market is predominately comprised of single-family home sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['1FamHomeSales'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FRED MAC MORT INT: -.5 to .5\n",
    "\n",
    "The box plot targets an index feature of the economic state, The Fred Mac Mort Int Rate.\n",
    "\n",
    "The conclusions are drawn on observations limtied to parcels with profitable sales records. \n",
    "\n",
    "The Fred Mac Mort Int observations are divided into 50 bins (contiguous ranges). Each occurance of a bin is tabulated and the counts are graphed. In this case, the bins that occur 75% or more of the time have between 3.5k-5k observations each, representing nearly the entire range of values -.66 to .3.\n",
    "\n",
    "Fred Mac Mort Int rates -.5 <= x <= .5 should be favored. The market favors stability in the mortgage interest rate, with a slight bias towards downward momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.qcut(parcels_w_sales[parcels_w_sales['Profitable Sale'] == 1]['FredMacMortInt'], q=50, duplicates='drop')\\\n",
    "    .value_counts()\\\n",
    "    .plot('box', figsize=(12,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Machine Learning Model (Supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import recall_score, precision_score, precision_recall_curve, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep backtest data: Cleanup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove buff dist from geometry\n",
    "parcels_w_sales.drop('buff_dist', axis=1, inplace=True)\n",
    "# set geometry back to unbuffered polygons\n",
    "parcels_w_sales = parcels_w_sales.set_geometry('geometry')\n",
    "# set homeowner to binary\n",
    "parcels_w_sales['homeowner'] = parcels_w_sales['homeowner'].replace({'N':0, 'Y':1, 'U': np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference unnecessary cols and prevent data leakage\n",
    "cols_to_drop =['st_areashape', 'district', 'geometry']\n",
    "# drop cols\n",
    "parcels_w_sales.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the property class to a numeric value with a lookup\n",
    "prop_class_lookup = {}\n",
    "# iterate through prop class vals and assign to dict\n",
    "for idx, i in enumerate(parcels_w_sales['property_class'].unique()):\n",
    "    prop_class_lookup[i] = idx\n",
    "# assign key lookup to property class col\n",
    "parcels_w_sales['property_class'] = parcels_w_sales['property_class'].apply(lambda x: prop_class_lookup.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts of property class to compare\n",
    "parcels_w_sales[parcels_w_sales['Profitable Sale']==1]['property_class'].value_counts().plot('bar');\n",
    "for key, val in prop_class_lookup.items():\n",
    "    print(val, ':', key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a clean index\n",
    "parcels_w_sales.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Sale Intvl to years\n",
    "parcels_w_sales['Sale Intvl'] = parcels_w_sales['Sale Intvl'] / pd.Timedelta(365, 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep Current Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remake the parcels link\n",
    "parcels_link = ''.join([HI_geo_stem, HI_parcels_json, '.geojson'])\n",
    "# load hi parcels as arcgis geodataframe \n",
    "hi_arc_parcels = arcgis.features.FeatureSet.from_geojson(requests.get(parcels_link).json()).df\n",
    "# remove qpub link from parcels\n",
    "hi_arc_parcels['qpub_link'] = hi_arc_parcels['qpub_link'].apply(lambda x: x[71:]).astype(int)\n",
    "hi_arc_parcels.rename({'qpub_link':'parcel'}, inplace=True, axis=1)\n",
    "# select only features supplied to classifier \n",
    "columns = ['bldgexempt', 'bldgvalue', 'gisacres',\n",
    "           'homeowner', 'landexempt', 'landvalue', \n",
    "           'pittcode', 'parcel', \n",
    "           'taxacres', 'SHAPE']\n",
    "hi_arc_parcels = hi_arc_parcels[columns]\n",
    "# convert dtypes for cols as necessary\n",
    "hi_arc_parcels['homeowner'] = hi_arc_parcels['homeowner'].replace({'N':0, 'Y':1, 'U': np.NaN})\n",
    "# add a Sale Date col to hi_arc_parcels\n",
    "hi_arc_parcels['Sale Date'] = pd.to_datetime('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## limit parcels to only the most recent record\n",
    "\n",
    "# limit columns in parcels_w_sales \n",
    "cols_to_drop = [\n",
    "    'bldgexempt', 'bldgvalue', 'gisacres', 'homeowner',\n",
    "    'landexempt', 'landvalue', 'pittcode', 'taxacres',\n",
    "    'Sale Amount', 'Profitable Sale', 'Sale Intvl',\n",
    "    \n",
    "    'Fed Funds', 'CaseShillMvt', 'ResConst', 'PrivConst', \n",
    "    'ConstPriceIdx', 'HiCtyJobless', 'PerCapInc', '1FamHomeSales', \n",
    "    'HiCtyHomeown', 'HPI US', 'HPI HI', 'FredMacMortInt'\n",
    "]\n",
    "# container to hold latest_sale history\n",
    "latest_sales = []\n",
    "# create an index of parcels from existing sales histroy \n",
    "to_do = parcels_w_sales['parcel'].unique().tolist()\n",
    "# sort all parcels by date\n",
    "parcels_w_sales.sort_values(['Sale Date'], ascending=False, inplace=True)\n",
    "\n",
    "# loop over sorted parcels with needed cols only\n",
    "for parcel in parcels_w_sales.drop(cols_to_drop, axis=1).iterrows():\n",
    "    # if parcel in remaining parcels\n",
    "    if parcel[1].loc['parcel'] in to_do:\n",
    "        # add this most recent record to stack\n",
    "        latest_sales.append(parcel[1])\n",
    "        # pop off of remaining parcels  \n",
    "        to_do.remove(parcel[1].loc['parcel'])\n",
    "\n",
    "# make dataframe from most recent parcels\n",
    "latest_sales = pd.DataFrame(latest_sales)\n",
    "# concat with hi_arc_parcels by calculating sale intvl\n",
    "final_sales = pd.concat([latest_sales, hi_arc_parcels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate only parcels where there are 2 records, one old, one new\n",
    "final_sales = final_sales[final_sales['parcel'].isin((final_sales['parcel'].value_counts() == 2).replace({False:np.NaN}).dropna().index)]\n",
    "# reset index\n",
    "final_sales.reset_index(inplace=True, drop=True)\n",
    "# sort the frame by parcel then Sale Date\n",
    "final_sales.sort_values(['parcel', 'Sale Date'], ascending=False, inplace=True)\n",
    "# calculate sale interval\n",
    "final_sales['Sale Intvl'] = final_sales['Sale Date'].diff(-1)\n",
    "# merge every 2nd record to get all records together: merge oldest with newest\n",
    "final_sales = pd.merge(final_sales.iloc[::2], final_sales.iloc[1::2], on='parcel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove cols with NAN\n",
    "cols_to_drop = [\n",
    "    'ELEV_MAX_x', \n",
    "    'ELEV_MEAN_x',\n",
    "    'ELEV_MIN_x',\n",
    "    'ELEV_RANGE_x',\n",
    "    'ELEV_STD_x',\n",
    "    'SLOPE_MAX_x',\n",
    "    'SLOPE_MEAN_x', \n",
    "    'SLOPE_MIN_x', \n",
    "    'SLOPE_RANGE_x', \n",
    "    'SLOPE_STD_x',\n",
    "    \n",
    "    'roads_nearby_x', \n",
    "    'schools_nearby_x', \n",
    "    'SHAPE_y', \n",
    "    'Sale Date_y', \n",
    "    'bldgexempt_y', \n",
    "    'bldgvalue_y',\n",
    "    'gisacres_y', \n",
    "    'homeowner_y', \n",
    "    'landexempt_y', \n",
    "    'landvalue_y', \n",
    "    'mkt_bld_val_x',\n",
    "    'mkt_land_val_x', \n",
    "    'pittcode_y', \n",
    "    'property_class_x', \n",
    "    'taxacres_y',\n",
    "    'tot_tax_val_x', \n",
    "    'Sale Intvl_y', # x | y\n",
    "    'zoning_id_x', \n",
    "    'land_area_x'\n",
    "]\n",
    "final_sales.drop(cols_to_drop, inplace=True, axis=1)\n",
    "# remove suffixes \n",
    "final_sales.columns = final_sales.columns.str.replace('(_y)|(_x)', '')\n",
    "# convert Sale Intvl to years\n",
    "final_sales['Sale Intvl'] = final_sales['Sale Intvl'].dt.days / 365\n",
    "# merge hi_arc_parcels with final_sales to convert to spatial df\n",
    "hi_arc_parcels = hi_arc_parcels.merge(final_sales, on='parcel')\n",
    "# free memory\n",
    "del(final_sales, columns, parcel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneeded cols by suffixes \n",
    "hi_arc_parcels.drop([col for col in hi_arc_parcels.columns if '_x' in col], inplace=True, axis=1)\n",
    "# remove suffixes in required cols\n",
    "hi_arc_parcels.columns = hi_arc_parcels.columns.str.replace('(_x)|(_y)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of present-day values\n",
    "present_day_indexes = {\n",
    "    'Fed Funds':getRecentIdxMovement('FRED/DFF', 'quandl'),\n",
    "    'CaseShillMvt':getRecentIdxMovement('CSUSHPINSA', 'fred'),\n",
    "    'ResConst':getRecentIdxMovement('FRED/TLRESCONS', 'quandl'),\n",
    "    'PrivConst':getRecentIdxMovement('FRED/TLPRVCONS', 'quandl'),\n",
    "    'ConstPriceIdx':getRecentIdxMovement('FRED/WPUSI012011', 'quandl'),\n",
    "    'HiCtyJobless':getRecentIdxMovement('FRED/HIHAWA0URN', 'quandl'),\n",
    "    'PerCapInc':getRecentIdxMovement('FRED/PCPI15001', 'quandl'),\n",
    "    'HiCtyHomeown':getRecentIdxMovement('HOWNRATEACS015001', 'fred'),\n",
    "    '1FamHomeSales':getRecentIdxMovement('FRED/HSN1F', 'quandl'),\n",
    "    'HPI US':getRecentIdxMovement('FRED/USSTHPI', 'quandl'),\n",
    "    'HPI HI':getRecentIdxMovement('FRED/HISTHPI', 'quandl'),\n",
    "    'FredMacMortInt':getRecentIdxMovement('FMAC/FIX30YR', 'quandl'),\n",
    "    }\n",
    "\n",
    "# broadcast values to present_day parcels\n",
    "for key in present_day_indexes.keys():\n",
    "    hi_arc_parcels[key] = present_day_indexes[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ML Model and Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test backtest data splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = parcels_w_sales[[i for i in parcels_w_sales.columns if i != 'Profitable Sale']]\n",
    "y = parcels_w_sales['Profitable Sale'] \n",
    "# fillna() for decision tree \n",
    "X.fillna(-99999999.0, inplace=True)\n",
    "# make train / test splits\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "del(parcels_w_sales, latest_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate classifier and fit with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Decision Tree classifier to capture feature importances\n",
    "clf = DecisionTreeClassifier(max_depth=10, random_state=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the training data, ignoring geometry, parcel, and Sale Date cols to prevent leakage\n",
    "mask =[i for i in Xtrain.columns if i != 'geometry' and i != 'parcel' and i != 'Sale Date' and i != 'Sale Amount']\n",
    "clf.fit(Xtrain[mask], ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the classifier, ignoring geometry, parcel, and Sale Date cols\n",
    "clf.score(Xtest[mask], ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture backtested predictions for scoring \n",
    "predictions = clf.predict(Xtest[mask])\n",
    "# capture accuracy and recall scores\n",
    "accuracy = np.round(clf.score(Xtest[mask], ytest) * 100, 2); print(accuracy)\n",
    "recall = np.round(recall_score(ytest, predictions) * 100, 2) ; print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the roc curve\n",
    "fpr, tpr, thresholds = roc_curve(ytest, predictions)\n",
    "# compute the area under receiver operator\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show feature importances\n",
    "These were the 10 most important features for predicting profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importances using indices to get column names\n",
    "sorted_feats = Xtest.columns[np.argsort(clf.feature_importances_)[-10:].tolist()]\n",
    "sorted_scores = np.sort(clf.feature_importances_)[-10:]\n",
    "# iterate through sorted values and print feature importance name and score\n",
    "for feat, score in zip(sorted_feats, sorted_scores):\n",
    "    print(feat+':', np.round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run on Current Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna() for decision tree\n",
    "hi_arc_parcels.fillna(.000000000000001, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_mask = [ 'bldgexempt', 'bldgvalue', 'gisacres',\n",
    "       'homeowner', 'landexempt', 'landvalue', 'pittcode', 'taxacres',\n",
    "       'Sale Intvl', 'ELEV_MAX', 'ELEV_MEAN',\n",
    "       'ELEV_MIN', 'ELEV_RANGE', 'ELEV_STD', 'SLOPE_MAX', 'SLOPE_MEAN',\n",
    "       'SLOPE_MIN', 'SLOPE_RANGE', 'SLOPE_STD', 'land_area', 'mkt_bld_val',\n",
    "       'mkt_land_val', 'property_class', 'roads_nearby', 'schools_nearby',\n",
    "       'tot_tax_val', 'zoning_id', 'Fed Funds', 'CaseShillMvt', 'ResConst',\n",
    "       'PrivConst', 'ConstPriceIdx', 'HiCtyJobless', 'PerCapInc',\n",
    "       'HiCtyHomeown', '1FamHomeSales', 'HPI US', 'HPI HI', 'FredMacMortInt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba on current data\n",
    "hi_arc_parcels['prediction'] = clf.predict_proba(hi_arc_parcels[[col for col in hi_arc_parcels.columns if col != 'SHAPE' and col != 'parcel' and col != 'Sale Date' and col != 'Sale Amount']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert parcel to string for display\n",
    "hi_arc_parcels['parcel'] = hi_arc_parcels['parcel'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit display selection \n",
    "hi_arc_parcels = hi_arc_parcels[['SHAPE', 'parcel', 'prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Results with ArcGIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import arcgis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate an ArcGIS instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a gis\n",
    "gis = arcgis.GIS(\"https://python.playground.esri.com/portal\", \"arcgis_python\", \"amazing_arcgis_123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backtested results in a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map widget and center on Hilo, Hawaii\n",
    "backtest_map = gis.map('Hilo, Hawaii', zoomlevel=9)\n",
    "backtest_map.basemap = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict that the observed data is True for profitability and assign predict_true to Xtest\n",
    "Xtest['prediction'] = clf.predict_proba(Xtest[mask])[:,1]\n",
    "# recombine train and test data\n",
    "Xtest['actual'] = ytest\n",
    "# get only desired cols for display\n",
    "cols = ['parcel', 'Sale Date', 'prediction', 'actual']\n",
    "# convert Sale Date to string for display\n",
    "Xtest['Sale Date'] = Xtest['Sale Date'].astype(str)\n",
    "# set cols\n",
    "Xtest = Xtest[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve latest parcel shapes one more time\n",
    "parcels_link = ''.join([HI_geo_stem, HI_parcels_json, '.geojson'])\n",
    "# load hi parcels as arcgis geodataframe \n",
    "arc_parcels = arcgis.features.FeatureSet.from_geojson(requests.get(parcels_link).json()).df\n",
    "# remove qpub link from parcels\n",
    "arc_parcels['qpub_link'] = arc_parcels['qpub_link'].apply(lambda x: x[71:]).astype(int)\n",
    "arc_parcels.rename({'qpub_link':'parcel'}, inplace=True, axis=1)\n",
    "# select only SHAPE col \n",
    "arc_parcels = arc_parcels[['parcel','SHAPE']]\n",
    "# convert to arcgis geodataframe, merging SHAPEs on parcel \n",
    "Xtest = arc_parcels.merge(Xtest, on='parcel', how='inner').drop('parcel', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the layer to the map\n",
    "backtest_map.add_layer(Xtest.to_featureset(), {\"renderer\":\"ClassedColorRenderer\", \"field_name\":\"prediction\"})\n",
    "backtest_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Current parcel predictions in a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map widget and center on Hilo, Hawaii\n",
    "current_map = gis.map('Hilo, Hawaii', zoomlevel=9)\n",
    "current_map.basemap = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the counts of predictions\n",
    "hi_arc_parcels['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add first 200 records where prediction is low (<=50%)\n",
    "current_map.add_layer(hi_arc_parcels[hi_arc_parcels['prediction'] <=.5].sort_values(['prediction'], ascending=False).iloc[:200].to_featureset(), {\"renderer\":\"ClassedColorRenderer\", \"field_name\":\"prediction\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the 200 lowest predictions ...this is where investors might find bargain opportunities\n",
    "current_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
